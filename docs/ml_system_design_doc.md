# ML System Design Doc - Рекомендация релевантной рекламы

---

## 1. Цели и предпосылки

### 1.1. Зачем идем в разработку продукта?

- **Бизнес-цель Product Owner**:  
  Платформа Avito.tech хочет повысить эффективность рекламных кампаний, предоставляя пользователям наиболее релевантные объявления. В результате этого предполагается увеличение CTR (Click-Through Rate) и улучшение ROI (Return on Investment) для рекламодателей.

- **Почему станет лучше, чем сейчас, от использования ML (Product Owner & Data Scientist)**:  
  Использование ML-моделей для предсказания вероятности клика обеспечит:
  - Персонализированные рекомендации, что повысит удовлетворенность пользователей.
  - Снижение расходов на неэффективные показы, что приведет к более высокому ROI для рекламодателей.

- **Что будем считать успехом итерации с точки зрения бизнеса (Product Owner)**:  
  Успехом будет считаться:
  - Достижение неплохой точности в предсказаниях модели (ROC AUC ≥ 0.55).
  - Увеличение CTR.
  - Уменьшение затрат на рекламу с учётом роста отклика пользователей.

---

### 1.2. Бизнес-требования и ограничения

- **Краткое описание БТ и ссылки на детальные документы с бизнес-требованиями (Product Owner)**:  
  Основное бизнес-требование — создание модели, предсказывающей вероятность клика по рекламному объявлению.

- **Бизнес-ограничения (Product Owner)**:  
  1. Модель должна работать с анонимизированными данными пользователей, не нарушая требования GDPR.
  2. Требования к скорости модели: предсказания должны быть сгенерированы в онлайне (за несколько десятков миллисекунд).
  3. Для первого запуска не требуется использование нейросетевых моделей, решение доложно работать без GPU.

- **Что мы ожидаем от конкретной итерации (Product Owner)**:  
  Итерация должна привести к созданию работающего прототипа модели, который будет оцениваться по метрике качества ROC-AUC. Модель должна продемонстрировать улучшение в точности предсказаний по сравнению с бейзлайном - если пользователь уже увидел и кликнул на рекламу - он кликнет снова.

- **Описание бизнес-процесса пилота (Product Owner)**:  
  Модель может быть использована в качестве модели первого уровня в общем пайплайне рекомендации рекламы для отсева кандидатов и генерации дополнительной фичи, которая поможет модели градиентного бустинга в ранжировании.

- **Что считаем успешным пилотом? Критерии успеха и возможные пути развития проекта (Product Owner)**:  
  Пилот считается успешным, если:
  - Побит порог ROC AUC ≥ 0.55 (цифра взята исходя из анализа лидерборда открытого соревнования, в котором использовались эти данные).
  - Для работы модели не требуется GPU.

---

### 1.3. Что входит в скоуп проекта/итерации, что не входит

- **На закрытие каких БТ подписываемся в данной итерации (Data Scientist)**:  
  В рамках итерации мы закрываем создание рабочей модели для предсказания кликов на основе данных о взаимодействиях пользователей с рекламой и характеристиках рекламных кампаний. Ожидается, что модель будет обучена на исторических данных и провалидирована.

- **Что не будет закрыто (Data Scientist)**:  
  - Модели для прогнозирования других метрик (например, ROI или eCPM) в рамках текущей итерации.
  - Интеграция модели с системой показа рекламы и API в реальном времени.

- **Описание результата с точки зрения качества кода и воспроизводимости решения (Data Scientist)**:  
  - Код должен соответствовать PEP8, иметь докстринги, и проходить проверки линтеров.
  - Решение будет воспроизводимым, с использованием контейнеров (например, Docker) для упрощения развертывания модели.

- **Описание планируемого технического долга (что оставляем для дальнейшей продуктивизации) (Data Scientist)**:  
  - Интеграция модели в рабочее окружение.
  - Настройка и автоматизация обновления модели с использованием новых данных.
  - Оптимизация модели для работы в реальном времени с более низкими задержками.

---

### 1.4. Предпосылки решения

- **Описание всех общих предпосылок решения, используемых в системе – с обоснованием от запроса бизнеса (Data Scientist)**:  
  - Данные о взаимодействиях пользователей с рекламой являются достаточными для предсказания вероятности клика.
  - Модель будет прогнозировать вероятность клика на основе исторических данных за последние 3 недели.
  - Модель будет работать на заранее определенном наборе пользователей и рекламных предложений.

---

## 2. Методология

### 2.1. Постановка задачи

Мы разрабатываем рекомендательную модель, которая будет предсказывать вероятность клика на рекламное объявление для каждого пользователя на основе:
- Исторических данных о взаимодействиях пользователя с рекламой.
- Характеристик рекламной кампании (например, цель кампании, категория, и т.д.).
  
Модель будет классификационной (бинарной), где цель — предсказать, кликнет ли пользователь на рекламу (класс 1) или нет (класс 0).

---

### 2.2. Блок-схема решения

![Блок-схема](/images/block-scheme.jpg)

### 2.3. Этапы решения задачи

#### Этап 1. Подготовка данных
**Описание данных и сущностей**  
Изначально данные представляют собой parquet-файлы с таблицами, содержащими данные о взаимодействиях пользователей с рекламой и характеристиках рекламных кампаний, и представлены следующим образом:

* Основная таблица с взаимодействиями:  
platform_id: id платформы (Android, Ios и т.п.)  
user_id: id пользователя   
adv_campaign_id: id рекламной кампании   
banner_code: код баннера  
adv_creative_id: индификатор креатива  
event_date: дата показа рекламной кампании пользователю  
is_main: показ рекламы был осуществлен с главной страницы   
target: клик / не клик   

* Категории:  
microcat_id: id микрокатегории   
level_id: id уровня в дереве микрокатегорий   
parent_microcat_id: id родительской микрокатегории  
logcat_id: id логической категории   
vertical_id: id вертикали   
category_id: id категории   

* Рекламные кампании:  
adv_campaign_id: id рекламной кампании  
start_date: date дата начала рекламной кампании  
end_date: date дата завершения рекламной кампании   
goal_cost: цена за клик на рекламу  
goal_budget: общий бюджет рекламной кампании   
logcat_id: id логической категории товаров из рекламной кампании  
location_ids: id локации, на которую рекламная кампания распространяется   

**Проблемы, выявленные в EDA**
* Пропуски в столбцах таблицы категорий рекламы
* Сложная иерархия категорий
* Отсутствие данных о содержании рекламных банеров
* Недостаточно подробное описание столбцов в таблицах (например, banner_code)
* Недостаточное описание терминов (вертикаль, логическая категория, микрокатегория)
* Данные представлены только за 3 недели
* Сильный дисбаланс меток классов (только 0.51% кликов)
* Возможно, потребуется использовать методы oversampling (например, SMOTE) или undersampling
* Для проведения EDA понадобилось больше 32 Гб оперативной памяти
* Большое количество данных модет потенциально привести к долгому времени обучения моделей и большим затратам оперативной памяти

**Процесс генерации данных**  
Данные были получены один раз, за первые 3 недели сентября 2024 года и были представлены на хакатоне. Повторной генерации данных не планируется.

**Объём данных**  
Данные содержат более 100M взаимодействий, 3M уникальных пользователей, 4K уникальных рекламных кампаний за 3 недели. Данного объёма должно быть достаточно для построения предсказаний на ближайшую неделю.

**Конфиденциальная информация**  
Данные анонимизированы (platform_id, user_id, adv_campaign_id, banner_code, adv_creative_id) и не нарушают требования GDPR.

**Необходимый результат этапа**
1. Изучение данных и выявление проблем:
   - Анализ распределений целевой переменной и основных фичей.
   - Проверка наличия выбросов.
   - Выявление коллинеарных признаков и потенциально избыточных фичей.
2. Нахождение потенциально важных признаков:
   - Исследование корреляций между признаками и целевой переменной.
   - Генерация сводных статистик (например, CTR для разных категорий, платформ, и дней недели).
3. Очистка данных от пропусков и дубликатов:
   - Удаление или заполнение пропусков (например, средними, медианами).
   - Удаление или корректировка дубликатов в данных.
4. Объединение данных в единую таблицу:
   - Слияние таблиц Train/Test, Campaigns, и Categories.
   - Проверка корректности связей между user_id, adv_campaign_id, и другими идентификаторами.
5. Создание новых признаков:
   - Генерация временных фичей (например, день недели, праздничный день).
   - Создание агрегированных признаков (например, средний CTR пользователя или кампании).
6. Создание необходимых скриптов для преобразования данных:
   - Разработка функций или классов для автоматизации обработки данных.
   - Сохранение пайплайна обработки данных для дальнейшего использования.
7. Оптимизация типов данных:
   - Приведение данных к более экономным типам (float32, int16, и т.д.).
   - Сокращение потребления памяти при загрузке данных.


#### Этап 2. Подготовка прогнозных моделей
##### ML-метрики и функции потерь

**Для бинарной классификации:**
- **ROC AUC**: Подходит для оценки качества классификации, особенно в условиях дисбаланса классов, позволяет оценить модель при разных порогах.
- **Precision, Recall и F1-score**: Полезны для разносторонней оценки качества классификации.
- **Weighted Binary Crossentropy**: Учитывает дисбаланс классов, задавая больший вес для меньшего класса.

**Для ранжирования:**
- **NDCG (Normalized Discounted Cumulative Gain)**: Учитывает положение релевантных элементов в списке, назначая больший вес верхним позициям. Это особенно важно в рекламных системах, где верхние позиции получают больше кликов. По сравнению с MAP, NDCG штрафует за неправильный порядок релевантных элементов, что делает её предпочтительной для задач с критичной важностью топовых позиций.
- **MRR (Mean Reciprocal Rank)**: Фокусируется на нахождении первого релевантного элемента, что полезно в задачах, где важен только первый клик. Однако она игнорирует качество остальных позиций, что делает её менее универсальной, чем NDCG.
- **MAP (Mean Average Precision)**: Эта метрика оценивает среднюю точность рекомендаций на всех позициях, но не учитывает их относительную важность (в отличие от NDCG).

**Функции потерь для ранжирования:**
- **LambdaRank Loss**: Учитывает относительную важность позиций в списке и оптимизирует NDCG, что делает его идеальным для задач с критичным значением топовых позиций.
- **Listwise функции потерь**: Оптимизируют весь список целиком, а не отдельные пары. Это делает его подходящим для сложных задач ранжирования, но увеличивает вычислительные затраты, что может быть ограничением при больших наборах данных.

##### Схема ML-валидации
- Данные разделяются на тренировочные и тестовые выборки по времени (`train` - первые 80%, `test` - последние 20%).
- Time-based CV для учета сезонных эффектов.

##### Структура бейзлайна
**Бейзлайн без ML:**
- Предположение: пользователь кликнет на ту же рекламу, на которую он кликал ранее. Результаты сравниваются с ML-моделями.

**Бейзлайн с ML:**
- **Модель**: Gradient Boosting (например, CatBoost, LightGBM, XGBoost).
- **Особенности**:
  - Применение взвешенной функции потерь для корректировки дисбаланса
  - Применение over- и under- сэмплинга для корректировки дисбаланса

##### Расширенные системы
**Система с ранжированием:**
- **Использование двухуровневого подхода**:
  - **Модели первого уровня (пример)**:
    - EASE
    - LightFM
    - SVD
    - iALS
    - User/Item KNN
    - VAE
    - SASRec / BERT4Rec
  - **Модель второго уровня**:
    - Gradient Boosting (например, LightGBM), использующий предсказания моделей первого уровня как фичи, а также дополнительные признаки из данных.

##### Стратегии развития системы:
- Улучшение ранжирования через интеграцию информации о категориях товаров и пользовательских предпочтений.
- Использование pre-trained моделей или векторных представлений контента.
- Исследование общих предпочтений пользователей и краткосрочных трендов. Адаптация трансформерной модели под это (Как в KuaiFormer)

##### Необходимый результат этапа
- Модель, достигающая порога качества ROC AUC ≥ 0.55.
- Выявление ключевых признаков, влияющих на предсказания.
- Построение двухуровневой системы для рекомендаций.

#### Этап 3. Важность признаков, интерпретируемость модели
- Выявление наиболее значимых признаков с использованием методов SHAP, Permutation Importance или Gini Importance (для моделей на основе деревьев).
- Оценка влияния каждого признака на предсказания модели, создание отчетов для бизнес-команды.
- Валидация важности признаков через удаление/добавление фичей и измерение изменения метрик качества.

#### Этап 4. Улучшение моделей, тюнинг гиперпараметров
- Проведение гиперпараметрической оптимизации, например, с использованием Optuna.
- Эксперименты с различными экспериментальными архитектурами системы (трансформеры на последнем уровне или LLM для выбора категории рекламы чтобы повысить diversity).
- Тестирование новых стратегий feature engineering, добавление новых и удаление избыточных фичей.
- Анализ устойчивости модели к изменению данных, изучение возможного переобучения.
- Оптимизация модели для работы в реальном времени с более низкими задержками.
- Анализ устойчивости модели для обучения на меньшем количестве данных.

#### Этап 5. Создание ML-сервиса и его тестирование
- Разработка API для инференса модели с использованием FastAPI или Flask.
- Контейнеризация сервиса с Docker, настройка CI/CD пайплайнов.
- Тестирование производительности сервиса в условиях высокой нагрузки.
- Настройка мониторинга предсказаний и логирования ошибок.

#### Этап 6. Автоматизация процесса обучения и обновления модели
- Настройка Airflow для автоматизации обучения модели по расписанию.
- Интеграция MLOps-инструментов, таких как MLflow, для управления экспериментами и версиями моделей.
- Валидация качества обновленных моделей перед их деплоем.

#### Этап 7. Закрытие технического долга
- Оптимизация кода для снижения затрат вычислительных ресурсов.
- Документирование всех компонентов системы, создание гайдлайнов для будущих разработчиков.
- Устранение выявленных технических проблем (например, долгой обработки данных или избыточного потребления памяти).